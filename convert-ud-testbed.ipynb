{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag.mapping import map_tag\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 tags in the universal dependencies dataset\n",
    "tags = ['VERB','NOUN','PRON','ADJ','ADV','ADP','CONJ','DET','NUM','PRT','X','.']\n",
    "# italian and spanish have a slightly modified version of UD, so this dictionary maps to the correct one\n",
    "u_pos_to_ud = {'PROPN':'NOUN', 'SCONJ':'CONJ', 'CCONJ':'CONJ', 'PUNCT':'.', 'PART':'PRT', 'AUX':'VERB', 'SYM':'.', '_':'X'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the languages have slightly different syntax to convert to UD\n",
    "def fix_pos(pos, language):\n",
    "    if language == 'english':\n",
    "        return map_tag('en-ptb', 'universal', pos)\n",
    "    elif language == 'german':\n",
    "        return map_tag('de-tiger', 'universal', pos.split('.')[0])\n",
    "    elif language == 'hungarian':\n",
    "        try:\n",
    "            return map_tag('hu-szeged', 'universal', pos.split('.')[0]+pos.split('.')[1]) \n",
    "        except:\n",
    "            return 'X'\n",
    "    elif language == 'italian':\n",
    "        modified_pos = pos.split('.')[0]\n",
    "        if modified_pos not in tags:\n",
    "            try:\n",
    "                return u_pos_to_ud[modified_pos]\n",
    "            except:\n",
    "                return 'X'\n",
    "        return modified_pos\n",
    "    elif language == 'polish':\n",
    "        return map_tag('pl-ipipan', 'universal', pos.split(':')[0])\n",
    "    elif language == 'portuguese':\n",
    "        return pos.split('.')[0]\n",
    "    elif language == 'spanish':\n",
    "        modified_pos = pos.split('.')[0]\n",
    "        if modified_pos not in tags:\n",
    "            return u_pos_to_ud[modified_pos]\n",
    "        return modified_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the languages where NLTK conversion works\n",
    "working_languages = ['english', 'german', 'hungarian', 'italian', 'polish', 'portuguese', 'spanish']\n",
    "# we'll store the POS tags in a list of lists\n",
    "master_pos_list = []\n",
    "for language in working_languages:\n",
    "    pos_list = []\n",
    "    with open('./tagfiles/Concatenated/_Cleaned/cleaned '+language+'.txt') as fhandle:\n",
    "        for line in fhandle:\n",
    "            if line.strip() != '': #remove all empty lines\n",
    "                pos = line.split('\\t')[1] # take the POS tag in the second column\n",
    "                modified_pos = fix_pos(pos, language) # run through NLTK\n",
    "                pos_list.append(modified_pos)\n",
    "    master_pos_list.append(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump into pickle file to be read by distance-clustering-testbed\n",
    "pickle.dump(master_pos_list, open('pos_tags.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENT\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m fhandle:\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m line\u001b[39m.\u001b[39msplit() \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m         pos \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m)[\u001b[39m1\u001b[39;49m]\n\u001b[1;32m     14\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m         modified_pos \u001b[39m=\u001b[39m modified_greek_dict[pos[\u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m]]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# greek_pos = []\n",
    "# \n",
    "# greek_dict = tagset_mapping('el-gdt', 'universal')\n",
    "# modified_greek_dict = {}\n",
    "# for pos_label in greek_dict.keys():\n",
    "#     modified_greek_dict[pos_label[0:2]] = greek_dict[pos_label]\n",
    "# \n",
    "# bad_words = []\n",
    "# \n",
    "# with open('./tagfiles/Concatenated/_Cleaned/cleaned greek.txt') as fhandle:\n",
    "#     for line in fhandle:\n",
    "#         if line.split() != '':\n",
    "#             pos = line.split('\\t')[1]\n",
    "#         try:\n",
    "#             modified_pos = modified_greek_dict[pos[0:2]]\n",
    "#         except:\n",
    "#             print(pos)\n",
    "#         greek_pos.append(modified_pos)\n",
    "# \n",
    "# print(set(bad_words))\n",
    "# print(greek_pos[0:20])\n",
    "# print('difference', list(set(german_pos).difference(tags)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc0e34c98fa48c414a72dad30aca4365d64be0603c09adfda1a83fce1973ae37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
